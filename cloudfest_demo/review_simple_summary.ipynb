{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_model = 'gemini-2.0-flash-001'\n",
    "git_repo_name = \"piotrhyzy/cloudfest2025\"\n",
    "git_pr_number = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configuration as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=config.VERTEX_PROJECT_ID, location=config.VERTEX_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.gcp import Secret\n",
    "secret = Secret.access_secret_version(\n",
    "        config.GITHUB_SECRET_PROJECT_ID,\n",
    "        config.GITHUB_SECRET_NAME,\n",
    "    )\n",
    "app_id = secret[\"app_id\"]\n",
    "installation_id = secret[\"installation_id\"]\n",
    "private_key = secret[\"private_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import get_client\n",
    "git_client = get_client(app_id, installation_id, private_key, config.GITHUB_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch PR Title and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import get_pr_details\n",
    "pr_details = get_pr_details(git_client, git_repo_name, git_pr_number)\n",
    "\n",
    "print(pr_details.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch PR Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import get_pr_diff\n",
    "pr_diff = get_pr_diff(git_client, git_repo_name, git_pr_number)\n",
    "\n",
    "print(pr_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request LLM For Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "\n",
    "def summarize_pull_request(pr_title: str, pr_description: str, pr_diff: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls the Gemini model on Vertex AI to generate a concise summary of a pull request\n",
    "    based on its title, description, and code diff.\n",
    "    Returns the generated summary as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare configuration for the model\n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": 8192, # limit the length of the response\n",
    "        \"temperature\": 0.5, # lower temperature for more deterministic output\n",
    "        \"top_p\": 1, # more deterministic output\n",
    "        # \"top_k\": 30, # more deterministic output\n",
    "    }\n",
    "    # Load the Gemini model\n",
    "    model = GenerativeModel(ai_model)\n",
    "\n",
    "    # Build the prompt in English (basic instructions, no explicit \"role\")\n",
    "    prompt_text = f\"\"\"\n",
    "        Below is a pull request:\n",
    "\n",
    "        Title:\n",
    "        {pr_title}\n",
    "\n",
    "        Description:\n",
    "        {pr_description}\n",
    "\n",
    "        Code Diff:\n",
    "        {pr_diff}\n",
    "\n",
    "        Task:\n",
    "        1. Read the pull request details and code diff.\n",
    "        2. Provide a concise summary of what this PR changes and why it matters with **Highlights**.\n",
    "        3. Keep it short, focusing on the main modifications and purpose.\n",
    "        4. Provide Changeog using expandalbe \"Changelog\" section for more details.\n",
    "\n",
    "       Summary of Changes:\n",
    "        \"\"\"\n",
    "\n",
    "    # Call the model with the prompt\n",
    "    response = model.generate_content(\n",
    "        prompt_text,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    response_text = response.text.strip()\n",
    "\n",
    "    # Strip the code fences if present\n",
    "    if response_text.startswith('```json'):\n",
    "        response_text = response_text[7:]\n",
    "    if response_text.endswith('```'):\n",
    "        response_text = response_text[:-3]\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "\n",
    "    # Return the model's output as a string\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_pull_request(pr_details.title, pr_details.description, pr_diff)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish Summary to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import post_comment\n",
    "post_comment(git_client, git_repo_name, git_pr_number, summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
