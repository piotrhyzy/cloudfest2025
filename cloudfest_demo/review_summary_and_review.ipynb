{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_model = 'gemini-2.0-flash-001'\n",
    "git_repo_name = \"piotrhyzy/cloudfest2025\"\n",
    "git_pr_number = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configuration as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=config.VERTEX_PROJECT_ID, location=config.VERTEX_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.gcp import Secret\n",
    "secret = Secret.access_secret_version(\n",
    "        config.GITHUB_SECRET_PROJECT_ID,\n",
    "        config.GITHUB_SECRET_NAME,\n",
    "    )\n",
    "app_id = secret[\"app_id\"]\n",
    "installation_id = secret[\"installation_id\"]\n",
    "private_key = secret[\"private_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import get_client\n",
    "git_client = get_client(app_id, installation_id, private_key, config.GITHUB_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch PR Title and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import get_pr_details\n",
    "pr_details = get_pr_details(git_client, git_repo_name, git_pr_number)\n",
    "\n",
    "print(pr_details.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch PR Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import get_pr_diff\n",
    "pr_diff = get_pr_diff(git_client, git_repo_name, git_pr_number)\n",
    "\n",
    "print(pr_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request LLM For Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "\n",
    "def summarize_pull_request(pr_title: str, pr_description: str, pr_diff: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls the Gemini model on Vertex AI to generate a concise summary of a pull request\n",
    "    based on its title, description, and code diff.\n",
    "    Returns the generated summary as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare configuration for the model\n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": 8192, # limit the length of the response\n",
    "        \"temperature\": 0.5, # lower temperature for more deterministic output\n",
    "        \"top_p\": 1, # more deterministic output\n",
    "        # \"top_k\": 30, # more deterministic output\n",
    "    }\n",
    "    # Load the Gemini model\n",
    "    model = GenerativeModel(ai_model)\n",
    "\n",
    "    # Build the prompt in English (basic instructions, no explicit \"role\")\n",
    "    prompt_text = f\"\"\"\n",
    "        Below is a pull request:\n",
    "\n",
    "        Title:\n",
    "        {pr_title}\n",
    "\n",
    "        Description:\n",
    "        {pr_description}\n",
    "\n",
    "        Code Diff:\n",
    "        {pr_diff}\n",
    "\n",
    "        Task:\n",
    "        1. Read the pull request details and code diff.\n",
    "        2. Provide a concise summary of what this PR changes and why it matters with Highlights.\n",
    "        3. Keep it short, focusing on the main modifications and purpose.\n",
    "        4. Provide Changeog using expandalbe \"Changelog\" section for more details.\n",
    "\n",
    "        Summary of Changes:\n",
    "        \"\"\"\n",
    "\n",
    "    # Call the model with the prompt\n",
    "    response = model.generate_content(\n",
    "        prompt_text,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    response_text = response.text.strip()\n",
    "\n",
    "    # Strip the code fences if present\n",
    "    if response_text.startswith('```json'):\n",
    "        response_text = response_text[7:]\n",
    "    if response_text.endswith('```'):\n",
    "        response_text = response_text[:-3]\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "\n",
    "    # Return the model's output as a string\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_summary = summarize_pull_request(pr_details.title, pr_details.description, pr_diff)\n",
    "print(pr_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import post_comment\n",
    "post_comment(git_client, git_repo_name, git_pr_number, pr_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk GitHub Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.diff import process_diff\n",
    "unified_diff = process_diff(pr_diff)\n",
    "print(unified_diff.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert thr strctured diff to a review File object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.review import convert_diff_to_review_files\n",
    "review_files = convert_diff_to_review_files(unified_diff, pr_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _review_file in review_files:\n",
    "    print(_review_file.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch File Snapschot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import download_file_snapschot_from_pr\n",
    "\n",
    "for _review_file in review_files:\n",
    "    _review_file.snapshot = download_file_snapschot_from_pr(git_client, git_repo_name, git_pr_number, _review_file.path)\n",
    "    print(_review_file.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request LLM For File Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "def review_single_file_changes_gemini(\n",
    "    file_path: str,\n",
    "    original_file_content: str,\n",
    "    diff: str,\n",
    "    language: str = 'python'\n",
    ") -> str:\n",
    "    \"\"\"Reviews changes in a single file using the Gemini Pro model via Vertex AI SDK.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the file.\n",
    "        original_file_content: The original content of the file.\n",
    "        diff: The diff of the changes.\n",
    "        language: The programming language of the code.\n",
    "\n",
    "    Returns:\n",
    "        The review of the changes as a string.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If there are issues with the input parameters or the API response.\n",
    "    \"\"\"\n",
    "\n",
    "    if not file_path:\n",
    "        raise ValueError(\"file_path is required\")\n",
    "    if not original_file_content:\n",
    "        raise ValueError(\"original_file_content is required\")\n",
    "    if not diff:\n",
    "        raise ValueError(\"diff is required\")\n",
    "    if not language:\n",
    "        raise ValueError(\"language is required\")\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    You are an expert software engineer tasked with reviewing a proposed code change.\n",
    "    Your role is to identify potential issues, suggest improvements,and assess the overall quality of the changes.\n",
    "\n",
    "    The context is a single file. You are provided with the file's original content, the changes made (in diff format), and the file's path.\n",
    "\n",
    "    **Instructions:**\n",
    "\n",
    "    1.  **Understand the Changes:** Carefully analyze the provided diff to understand what modifications have been made to the file.\n",
    "    2.  **Consider the Context:** Use the original file content to gain a better understanding of the changes in their existing codebase.\n",
    "    3.  **Focus on Code Quality:** Evaluate the changes with respect to:\n",
    "        *   **Correctness:** Do the changes function as intended? Are there any bugs or logical flaws?\n",
    "        *   **Readability:** Is the code easy to understand? Are variable names meaningful? Is the code well-formatted?\n",
    "        *   **Maintainability:** Are the changes maintainable? Are they well-structured?\n",
    "        *   **Efficiency:** Are there any performance bottlenecks or potential inefficiencies introduced?\n",
    "        *   **Security:** Are there any security vulnerabilities introduced?\n",
    "        *   **Best Practices:** Do the changes adhere to best practices for the {language} programming language?\n",
    "        * **Completeness** Are the changes complete? Is there missing logic?\n",
    "    4.  **Provide Specific Feedback:**\n",
    "        *   If you identify any issues, be specific about their location (e.g., \"Line 25: The variable name 'x' is not descriptive.\").\n",
    "        *   Provide concrete suggestions for improvement.\n",
    "        *   Explain the rationale behind your feedback.\n",
    "    5.  **Be Concise:** Keep your review focused and avoid unnecessary filler.\n",
    "    6. Be polite and respectful.\n",
    "\n",
    "    **Input:**\n",
    "\n",
    "    *   **File Path:** {file_path}\n",
    "    *   **Language:** {language}\n",
    "    *   **Original File Content:**\n",
    "    ```\n",
    "    {original_file_content}\n",
    "    ```\n",
    "    *   **Diff:**\n",
    "    ```\n",
    "    {diff}\n",
    "    ```\n",
    "\n",
    "    **Output:**\n",
    "\n",
    "    Provide a comprehensive code review of the changes, in markdown, following the instructions above.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        file_path=file_path,\n",
    "        original_file_content=original_file_content,\n",
    "        diff=diff,\n",
    "        language=language,\n",
    "    )\n",
    "\n",
    "    # Configure the Gemini Pro model\n",
    "    generation_config = {\n",
    "        \"candidate_count\": 1,\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 40,\n",
    "    }\n",
    "    # Load the Gemini model\n",
    "    model = GenerativeModel(ai_model)\n",
    "\n",
    "    # Call the model with the prompt\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    response_text = response.text.strip()\n",
    "\n",
    "    # Return the model's output as a string\n",
    "    return response.text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_reviews = []\n",
    "for _review_file in review_files:\n",
    "    file_reviews.append(\n",
    "        review_single_file_changes_gemini(original_file_content=_review_file.snapshot, diff=_review_file.diff, file_path=_review_file.path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([file_reviews])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Reviews into a single review summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_merged_files_changes(\n",
    "    reviews: list[str],\n",
    ") -> str:\n",
    "    \"\"\"Reviews changes in multiple files using the Gemini Pro model via Vertex AI SDK.\n",
    "\n",
    "    Args:\n",
    "        reviews: A list of code review strings (output from review_single_file_changes_gemini).\n",
    "\n",
    "    Returns:\n",
    "        A merged review summary as a string.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If there are issues with the input parameters or the API response.\n",
    "    \"\"\"\n",
    "\n",
    "    if not reviews:\n",
    "        raise ValueError(\"reviews list is empty\")\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    You are an expert software engineer tasked with reviewing a set of code changes across multiple files.\n",
    "    You have received individual code reviews for each file modified in this pull request. Your role is to:\n",
    "\n",
    "    1.  **Synthesize Information:** Combine the feedback from all the individual file reviews into a coherent overall assessment of the pull request.\n",
    "    2.  **Identify Major Changes:** Highlight the key changes across all the files.\n",
    "    3.  **Assess Overall Quality:** Provide an overall assessment of the quality of the changes in the pull request, including:\n",
    "        *   **Correctness:** Are the changes likely to work as intended, across all files?\n",
    "        *   **Readability:** Is the code easy to read and understand across all the files?\n",
    "        *   **Maintainability:** Are the changes maintainable across all the files?\n",
    "        *   **Efficiency:** Are there any performance issues across all the files?\n",
    "        *   **Security:** Are there any security issues across all the files?\n",
    "        *   **Completeness:** Is there any missing logic across all the files?\n",
    "    4.  **Provide Merge Assessment:** Based on your analysis, provide a recommendation on whether these changes are ready to be merged, or if further work is needed.\n",
    "    5. **Format Output for Github:** return a text in a nice format understandable by github (using markdown, bold, italic, emoji, ...).\n",
    "    6. Be polite and respectful.\n",
    "\n",
    "    **Input:**\n",
    "\n",
    "    *   **Individual File Reviews:**\n",
    "    {reviews_text}\n",
    "\n",
    "    **Output:**\n",
    "\n",
    "    Provide a comprehensive summary review of the changes across all files, in markdown, following the instructions above.\n",
    "    Include sections for:\n",
    "    *   **Major Changes**\n",
    "    *   **Issues**\n",
    "    *   **Overall Quality Assessment**\n",
    "    *   **Merge Assessment**\n",
    "\n",
    "    Provide only output no other comments or intorductions start with:\n",
    "\n",
    "    # Review of Changes:\n",
    "    \"\"\"\n",
    "\n",
    "    reviews_text = \"\\n\".join(\n",
    "        [f\"```\\n{review}\\n```\" for review in reviews]\n",
    "    )  # Format each review nicely\n",
    "\n",
    "    prompt = prompt_template.format(reviews_text=reviews_text)\n",
    "\n",
    "    # Configure the Gemini Pro model\n",
    "    generation_config = {\n",
    "        \"candidate_count\": 1,\n",
    "        \"max_output_tokens\": 4096,  # Increased max output tokens\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 40,\n",
    "    }\n",
    "    # Load the Gemini model\n",
    "    model = GenerativeModel(ai_model)\n",
    "\n",
    "    # Call the model with the prompt\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    response_text = response.text.strip()\n",
    "\n",
    "    # Return the model's output as a string\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_reviews_text =review_merged_files_changes(file_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_reviews_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish Summary to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.git import post_comment\n",
    "post_comment(git_client, git_repo_name, git_pr_number, file_reviews_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
